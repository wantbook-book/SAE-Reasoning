# Model Configuration
model_name: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
model_from_pretrained_kwargs:
  center_writing_weights: false
  dtype: bfloat16

# Dataset Configuration
dataset_path: andreuka18/DeepSeek-R1-Distill-Llama-8B-lmsys-openthoughts-tokenized
is_dataset_tokenized: true  # deprecated
prepend_bos: false
streaming: false

# SAE Configuration
architecture: standard
hook_name: blocks.19.hook_resid_post
hook_layer: 19
d_in: 4096
expansion_factor: 16
mse_loss_normalization: null
b_dec_init_method: zeros
apply_b_dec_to_input: false
normalize_sae_decoder: false
scale_sparsity_penalty_by_decoder_norm: true
decoder_heuristic_init: true
init_encoder_as_decoder_transpose: true
normalize_activations: expected_average_only_in

# Training Parameters
training_tokens: 819200000  # 200k steps
train_batch_size_tokens: 4096
lr: !!float 5e-5
adam_beta1: 0.9
adam_beta2: 0.999
lr_scheduler_name: constant
lr_warm_up_steps: 0
lr_decay_steps: 40000  # 20% of training (NOTE: This is measured in steps: 0.2 * training_tokens // train_batch_size_tokens)
l1_coefficient: 5
l1_warm_up_steps: 10000  # 5% of training (NOTE: This is measured in steps: 0.05 * training_tokens // train_batch_size_tokens)
lp_norm: 1.0  # l1 penalty
context_size: 1024

# Activation Storage Parameters
n_batches_in_buffer: 32
store_batch_size_prompts: 8

# Resampling Protocol (not used)
use_ghost_grads: false  # we don't use ghost grads anymore.
feature_sampling_window: 1000  # this controls our reporting of feature sparsity stats
dead_feature_window: 1000  # would effect resampling or ghost grads if we were using it.
dead_feature_threshold: !!float 1e-4  # would effect resampling or ghost grads if we were using it.

# WANDB
log_to_wandb: true
# wandb_project: "sae-openthoughts"
wandb_project: "sae-lmsys-openthoughts"
run_name: "r1-distill-llama-8b-ctx-1024-l1-5"
wandb_log_frequency: 30
eval_every_n_wandb_logs: 20
# MISC
device: cuda
seed: 42
n_checkpoints: 5
checkpoint_path: sae_runs
dtype: float32
exclude_special_tokens: [128000, 128001]
