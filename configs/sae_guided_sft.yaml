# check ./resee/utils/constant.py
seed: 42
sae_release: andreuka18/deepseek-r1-distill-llama-8b-lmsys-openthoughts
sae_id: blocks.19.hook_resid_post
sae_intervention_config: "{\"feature_idx\": 24648, \"max_activation\": 14.844, \"strength\": 1.0}"
sae_hook_layer: 19
sae_hook_point: blocks.19.hook_resid_post
sae_type: pretrained

base_model_name: /pubshare/LLM/deepseek-ai/DeepSeek-R1-Distill-Llama-8B
elicitation_dataset_name: /pubshare/fwk/code/sae/SAE-Reasoning2/sft/dataset/rlpr/rlpr_train_format.jsonl

lora_r: 32
lora_alpha: 128
lora_dropout: 0.05
lora_target_modules:
- q_proj
- k_proj
- v_proj
- o_proj
- down_proj
- up_proj
- gate_proj
learning_rate: 1e-6
num_epochs: 2
batch_size: 1
logging_steps: 1
save_steps: 500

ckpt_dir: /pubshare/fwk/code/sae/SAE-Reasoning2/sft/ckpts
log_dir: /pubshare/fwk/code/sae/SAE-Reasoning2/sft/logs
